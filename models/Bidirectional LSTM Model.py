# -*- coding: utf-8 -*-
"""Bidirectional LSTM Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15E7MD7GHB1P5EVb3Bnzo8DCUQvE1nZs6
"""

import numpy as np
import os
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
print(tf.test.is_gpu_available())

train_data, test_data = tfds.load(name="imdb_reviews", split=["train", "test"], 
                                  batch_size=-1, as_supervised=True)

train_examples, train_labels = tfds.as_numpy(train_data)
test_examples, test_labels = tfds.as_numpy(test_data)

print("Training entries: {}, test entries: {}".format(len(train_examples), len(test_examples)))
train_examples[:2]

train_labels[:2]

from tqdm import tqdm
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
import numpy as n

def load_data(examples, targets, num_words, sequence_length, test_size=0.25, oov_token=None):

    reviews, labels = [], []

    for example, label in zip(examples, targets):
      reviews.append(str(example).strip())
      labels.append(str(label).strip())

    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)
    tokenizer.fit_on_texts(reviews)
    X = tokenizer.texts_to_sequences(reviews)
    X, y = np.array(X), np.array(labels)
    X = pad_sequences(X, maxlen=sequence_length)

    # convert labels to one-hot encoded
    y = to_categorical(y)


    data = {}
    data["X_train"] = X
    data["y_train"] = y
    data["tokenizer"] = tokenizer
    data["int2label"] =  {0: "negative", 1: "positive"}
    data["label2int"] = {"negative": 0, "positive": 1}

    return data

data = load_data(train_examples, train_labels, 10000, 100)

class TextBiRNN(tf.keras.Model):
    """构建TextBiRNN模型"""
    def __init__(self,
                 word_index,
                 maxlen,
                 vocab_size,
                 embedding_dims,
                 num_class=2,

                 ):
        super(TextBiRNN, self).__init__()
        
        #embedding_matrix = get_embedding_vectors(word_index, embedding_dims)
        self.embed = tf.keras.layers.Embedding(len(word_index) + 1, 
                                               embedding_dims, 
                                               input_length=maxlen)
        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))
        self.dense = tf.keras.layers.Dense(64, activation='relu')
        self.logits = tf.keras.layers.Dense(num_class)

    def call(self, inputs):
        x = self.embed(inputs)
        x = self.bilstm(x)
        x = self.dense(x)
        x = self.logits(x)
        return x

model = TextBiRNN(data["tokenizer"].word_index,  300, 10000, 300)

model.compile(optimizer='adam',
              loss=tf.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

import os
# create these folders if they does not exist
if not os.path.isdir("results"):
    os.mkdir("results")
if not os.path.isdir("logs"):
    os.mkdir("logs")
if not os.path.isdir("data"):
    os.mkdir("data")
# load the data


model_name ="IMDB"

checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# 在文件名中包含 epoch (使用 `str.format`)
checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# 创建一个回调，每 5 个 epochs 保存模型的权重
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path, 
    verbose=1, 
    save_weights_only=True,
    period=5)

# using tensorboard on 'logs' folder
tensorboard = TensorBoard(log_dir=os.path.join("logs", model_name))
# start training
history = model.fit(data["X_train"], data["y_train"],
                    batch_size=256,
                    epochs=20,
                    callbacks=[cp_callback],
                    verbose=1)

#tf.saved_model.save(model,'my_saved_model')
#model.save_weights('./checkpoints/my_checkpoint')

new_model = TextBiRNN(data["tokenizer"].word_index,  300, 10000, 300)

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest

new_model.load_weights(latest)

test_data = load_data(test_examples, test_labels, 10000, 100)

# 重新评估模型
new_model.compile(optimizer='adam',
              loss=tf.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
loss, acc = new_model.evaluate(data["X_train"],  data["y_train"], verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100*acc))

def get_predictions(text):
    sequence = data["tokenizer"].texts_to_sequences([text])
    # pad the sequences
    sequence = pad_sequences(sequence, maxlen=300)
    # get the prediction
    prediction = new_model.predict(sequence)[0]
    return prediction, data["int2label"][np.argmax(prediction)]

text = "The movie is awesome!"
output_vector, prediction = get_predictions(text)
print("Output vector:", output_vector)
print("Prediction:", prediction)

