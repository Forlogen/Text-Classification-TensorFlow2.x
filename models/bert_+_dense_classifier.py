# -*- coding: utf-8 -*-
"""Bert  + Dense Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j2hljTQkWuRnfbBzDhOjRaOdRiHEkd0K
"""

!pip install transformers

import numpy as np
import os
import tensorflow as tf
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
print(tf.test.is_gpu_available())

train_data, test_data = tfds.load(name="imdb_reviews", split=["train", "test"], 
                                  batch_size=-1, as_supervised=True)

train_examples, train_labels = tfds.as_numpy(train_data)
test_examples, test_labels = tfds.as_numpy(test_data)

print("Training entries: {}, test entries: {}".format(len(train_examples), len(test_examples)))
train_examples[:2]

train_labels[:2]

from tqdm import tqdm
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import TensorBoard


def load_data(examples, targets, tokenizer):

    reviews, labels = [], []

    for example, label in zip(examples, targets):
      reviews.append(str(example).strip())
      labels.append(label)

    X = tokenizer(reviews, 
                       padding=True, 
                       truncation=True, 
                       return_tensors="tf")

    y = np.array(labels)
    
    print ("begin loding data...")
    data = {}
    data["X"] = X
    data["input_ids"] = X["input_ids"]
    data["token_type_ids"] = X["token_type_ids"]
    data["attention_mask"] = X["attention_mask"]
    data["y"] = y
    data["tokenizer"] = tokenizer
    data["int2label"] =  {0: "negative", 1: "positive"}
    data["label2int"] = {"negative": 0, "positive": 1}

    return data

from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel

tokenizer =  BertTokenizer.from_pretrained('bert-base-uncased')
bert = TFBertModel.from_pretrained("bert-base-uncased")

def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):
  return {
      "input_ids": input_ids,
      "token_type_ids": token_type_ids,
      "attention_mask": attention_masks,
  }, label

train_data = load_data(train_examples[:100], train_labels, tokenizer)
test_data = load_data(test_examples[:100], test_labels, tokenizer)

class Model:
    
    def __init__(self, bert, max_seq_len):
        
        self.bert = bert
        self.seq_max_len = max_seq_len
    
    def _create_model(self):
        encoder = self.bert
        
        input_ids = tf.keras.layers.Input(shape=(self.seq_max_len,), dtype=tf.int32)
        token_type_ids = tf.keras.layers.Input(shape=(self.seq_max_len,), dtype=tf.int32)
        attention_mask= tf.keras.layers.Input(shape=(self.seq_max_len,), dtype=tf.int32)
        outputs = encoder([input_ids, attention_mask, token_type_ids], training = True)
       
        logits = outputs[1]
        logits = tf.keras.layers.Dense(units=2, activation="softmax",use_bias=True)(logits)
        model = tf.keras.Model(
            inputs=[input_ids, attention_mask, token_type_ids],
            outputs=logits
        )
        
        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
        optimizer = tf.keras.optimizers.Adam(lr=1e-5)
        model.compile(optimizer=optimizer, loss=loss)
        
        return model

model = Model(bert=bert, max_seq_len=512)
classifier = model._create_model()
classifier.summary()

def create_learning_rate_scheduler(max_learn_rate=5e-5,
                                   end_learn_rate=1e-7,
                                   warmup_epoch_count=10,
                                   total_epoch_count=90):

    def lr_scheduler(epoch):
        if epoch < warmup_epoch_count:
            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)
        else:
            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))
        return float(res)
    
    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)

    return learning_rate_scheduler

if not os.path.isdir("logs"):
    os.mkdir("logs")

tensorboard = TensorBoard(log_dir=os.path.join("logs", "IMDB"))

total_epoch_count = 1
history = classifier.fit(x = [train_data['input_ids'], train_data["token_type_ids"], train_data["attention_mask"]], y = train_data["y"],
                    batch_size=2,
                    shuffle=True,
                    validation_split=0.1,
                    epochs=total_epoch_count,verbose = 1,
                    callbacks=[tensorboard])

!mkdir -p saved_model
bert.save_pretrained('saved_model/my_model')

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

new_model = Model(bert=bert, max_seq_len=512)._create_model()


new_model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5), 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False))
new_model.summary()

result = new_model.predict([test_data['input_ids'], test_data["token_type_ids"], test_data["attention_mask"]])
prediction = result
print('prediction is: {}'.format(prediction))

for p in prediction:
  p = tf.nn.softmax(p, axis=-1)
  print(test_data["int2label"][np.argmax(p)])

